{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9457bccf-00aa-4dca-be70-9b07584264ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "1a2c4e93-901d-4331-81ff-92316fe7c32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imdb = keras.datasets.imdb\n",
    "# num_words는 몇 개의 단어를 사용할 것인지 지정한다.\n",
    "# 훈련 데이터에서 가장 많이 등장하는 상위 10,000개의 단어를 선택한다.\n",
    "(x_train, y_train) , (x_test, y_test) = imdb.load_data(num_words=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "5428588d-61e6-4d19-b6a6-3a7603039264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n",
      "(25000,)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "4c5bcadd-7408-4e77-b3ff-5d4570705651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]\n"
     ]
    }
   ],
   "source": [
    "# 정수들이 출력되는 것을 볼 수 있는데,이것은 단어 임베딩이 아니다.\n",
    "# 임베딩은 부동소수점수로 된 벡터라는 것을 잊지 말자\n",
    "# imdb 데이터들은 미리 전처리가 되어 있다.\n",
    "# 즉 단어들을 토큰화하고 이것을 정수 인덱스로 인코딩한 상태이다.\n",
    "# 정수 인덱스는 각 단어들의 출현 회수에 따라 결정된다.\n",
    "# 즉 더 많이 등장하는 단어가 낮은 인덱스가 된다.\n",
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "3c759667-d812-4300-815f-deadd9e08e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(218, 189)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0]), len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "41b15f65-9774-4a85-8af8-85607ed9d713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 0)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0], y_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "86d04a82-ec37-4281-9d01-ef750adf1d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1], dtype=int64), array([12500, 12500], dtype=int64))"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train,return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2bad0fdc-abf4-4697-b361-bd4cc103a532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 -> 정수 인덱스 딕셔너리\n",
    "word_to_index = imdb.get_word_index()\n",
    "\n",
    "# 처음 몇 개의 인덱스는 특수 용도로 사용된다.\n",
    "## 딕셔너리의 모든 키값에 +3 씩한다\n",
    "word_to_index = {k:(v+3) for k,v in word_to_index.items()}\n",
    "## 몇몇개의 특수한 용도로 쓰이는 것을 추가한다.\n",
    "word_to_index[\"<PAD>\"] = 0\n",
    "word_to_index[\"<START>\"] = 1\n",
    "word_to_index[\"<UNK>\"] = 2\n",
    "word_to_index[\"<UNUSED>\"] = 3\n",
    "\n",
    "# 딕셔너리의 아이탬을 서로 바꿔준다.\n",
    "index_to_word = dict([(value, key) for (key, value) in word_to_index.items()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "1fb3db91-f35e-400e-8f9b-6d14ddcaad56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<START> this film was just brilliant casting location scenery story direction everyone's really suited the part they played and you could just imagine being there robert <UNK> is an amazing actor and now the same being director <UNK> father came from the same scottish island as myself so i loved the fact there was a real connection with this film the witty remarks throughout the film were great it was just brilliant so much that i bought the film as soon as it was released for <UNK> and would recommend it to everyone to watch and the fly fishing was amazing really cried at the end it was so sad and you know what they say if you cry at a film it must have been good and this definitely was also <UNK> to the two little boy's that played the <UNK> of norman and paul they were just brilliant children are often left out of the <UNK> list i think because the stars that play them all grown up are such a big profile for the whole film but these children are amazing and should be praised for what they have done don't you think the whole story was so lovely because it was true and was someone's life after all that was shared with us all\n"
     ]
    }
   ],
   "source": [
    "# join 연산을 통해 리스트에 있는 항목들을 다 붙여준다.\n",
    "print(' '.join([index_to_word[index] for index in x_train[0]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8f60527b-fe8b-45ae-9465-9d74d783adc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리\n",
    "## 정수 배열로 되어 있는 것을 부동소수점수를 가지고 있는 텐서로 변환되어야 한다.\n",
    "## 우리가 학습했다시피 변환하는 방법에는 몇 가지가 잇는데\n",
    "## 원-핫 코딩 : 정수 배열을 0과 1로 이루어진 벡터로 변환한다.\n",
    "## 예를 들어 배열 [3, 7]을 인덱스 3과 7만 1이고 나머지는 모두 0 인 10,000 차원 벡터로 변환할 수 있다.\n",
    "## 입력층으로는 실수 벡터 데이터를 다룰 수 있는 Dense 층을 신경망의 첫 번째 레이어로 사용한다\n",
    "## 이 방법은 num_words * num_reviews 크기의 행렬이 필요하기 때문에 메모리를 많이 사용한다.\n",
    "## 임베딩 방법 : 정수 배열이 길이가 모두 같도록 패딩(padding)을 추가해 max_length * (embedding) 레이어를\n",
    "## 신경망의 첫 번째 레이어로 사용할 수 있다. 임베딩 레이어에서 정수 배열을 부동소수점 벡터로 변환한다.\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "745a19bb-35b0-4a9f-9c2d-17b8dcaef593",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = pad_sequences(x_train, maxlen=100)\n",
    "x_test = pad_sequences(x_test, maxlen=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "096c30d9-2bf7-4d11-994c-0bf7bde4ef75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[0]), len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "73a58f4d-2c8a-49b7-95ca-b5471c4291aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1415   33    6   22   12  215   28   77   52    5   14  407   16   82\n",
      "    2    8    4  107  117 5952   15  256    4    2    7 3766    5  723\n",
      "   36   71   43  530  476   26  400  317   46    7    4    2 1029   13\n",
      "  104   88    4  381   15  297   98   32 2071   56   26  141    6  194\n",
      " 7486   18    4  226   22   21  134  476   26  480    5  144   30 5535\n",
      "   18   51   36   28  224   92   25  104    4  226   65   16   38 1334\n",
      "   88   12   16  283    5   16 4472  113  103   32   15   16 5345   19\n",
      "  178   32]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2db9d7b9-714f-412a-bccd-cb753d8965bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6400)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                409664    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,049,729\n",
      "Trainable params: 1,049,729\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 신경망 구축\n",
    "vocab_size = 10000\n",
    "\n",
    "model = Sequential()\n",
    "# Embedding 층은 인코딩된 단어를 입력받고 각 단어 인덱스에 해당하는 임베딩 벡터를 찾는다.\n",
    "# 첫 번째 인수는 단어 집합의 크기로 여기서는 10000으로 하였다. 두 번째 인수는 임베딩 후의 벡터크기로서\n",
    "# 64으로 설정하였다. 따라서 리뷰 데이터에서 모든 단어는 64차원의 임베딩 벡터로 표현된다. 이 벡터는 모델이\n",
    "# 훈련되면서 학습된다. 리뷰의 최대 길이를 100으로 제한하였다.\n",
    "model.add(Embedding(vocab_size, 64, input_length=100))\n",
    "# 이 고정 길이의 출력 벡터는 64개의 은닉 노드를 가진 FC 층을 거친다.\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "# 마지막 층은 이진 분류이므로 뉴런 하나로 되어 있고 시그모이드 활성화 함수를 사용하여 0과 1 사이의 실수를 출력한다.\n",
    "# 이 값은 확률 또는 신뢰도를 나타낸다.\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "db22e274-8e83-4dc6-bed4-f5f328541bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9a00fe96-697f-4e1c-8a4f-5725253c0a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 5s 6ms/step - loss: 0.4575 - accuracy: 0.7672 - val_loss: 0.3340 - val_accuracy: 0.8515\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.1673 - accuracy: 0.9398 - val_loss: 0.3970 - val_accuracy: 0.8358\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0247 - accuracy: 0.9945 - val_loss: 0.5329 - val_accuracy: 0.8344\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0045 - accuracy: 0.9998 - val_loss: 0.6012 - val_accuracy: 0.8394\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0023 - accuracy: 0.9997 - val_loss: 0.6514 - val_accuracy: 0.8379\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 8.4364e-04 - accuracy: 1.0000 - val_loss: 0.6952 - val_accuracy: 0.8379\n",
      "Epoch 7/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 5.7167e-04 - accuracy: 1.0000 - val_loss: 0.7241 - val_accuracy: 0.8402\n",
      "Epoch 8/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 3.2593e-04 - accuracy: 1.0000 - val_loss: 0.7524 - val_accuracy: 0.8404\n",
      "Epoch 9/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.6180e-04 - accuracy: 1.0000 - val_loss: 0.7793 - val_accuracy: 0.8410\n",
      "Epoch 10/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.8793e-04 - accuracy: 1.0000 - val_loss: 0.8023 - val_accuracy: 0.8403\n",
      "Epoch 11/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.3471e-04 - accuracy: 1.0000 - val_loss: 0.8322 - val_accuracy: 0.8390\n",
      "Epoch 12/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 1.2203e-04 - accuracy: 1.0000 - val_loss: 0.8581 - val_accuracy: 0.8396\n",
      "Epoch 13/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 9.9005e-05 - accuracy: 1.0000 - val_loss: 0.8813 - val_accuracy: 0.8394\n",
      "Epoch 14/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.8881e-05 - accuracy: 1.0000 - val_loss: 0.9083 - val_accuracy: 0.8401\n",
      "Epoch 15/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 7.8377e-05 - accuracy: 1.0000 - val_loss: 0.9323 - val_accuracy: 0.8406\n",
      "Epoch 16/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 5.2942e-05 - accuracy: 1.0000 - val_loss: 0.9587 - val_accuracy: 0.8411\n",
      "Epoch 17/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 4.3678e-05 - accuracy: 1.0000 - val_loss: 0.9797 - val_accuracy: 0.8403\n",
      "Epoch 18/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 3.5310e-05 - accuracy: 1.0000 - val_loss: 1.0075 - val_accuracy: 0.8401\n",
      "Epoch 19/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 2.6876e-05 - accuracy: 1.0000 - val_loss: 1.0342 - val_accuracy: 0.8407\n",
      "Epoch 20/20\n",
      "391/391 [==============================] - 2s 5ms/step - loss: 0.0225 - accuracy: 0.9921 - val_loss: 0.9841 - val_accuracy: 0.8115\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=64, epochs=20, verbose=1,\n",
    "                    validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a56b1f8-e42a-414b-9806-3fce365ffa34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 2s - loss: 0.9841 - accuracy: 0.8115 - 2s/epoch - 2ms/step\n",
      "[0.9840518236160278, 0.811519980430603]\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(x_test, y_test, verbose=2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cd597c48-7348-46f0-9fc9-3910d2a0dc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 'Aside from the super obvious plot holes and very poor story overall, the dragged-out unnecessary dialogue made this film unbearable and extremely boring. The way too long 1h 39min film length felt like 4 hours and I found myself saying \"get on with it already, who cares!\" when the two leads would just ramble on about nothing relevant. This movie may have been interesting if it was a 30 min short filmle and extremely boring. The way too long 1h 39min film length felt like 4 hours and I found myself saying \"get on with it already, who cares!\" when the two leads would just ramble on about nothing relevant. This movie may have been interesting if it was a 30 min short film (which oddly enough is the only minimal writing experience Adam Gaines has'\n",
    "\n",
    "import re\n",
    "review = re.sub(\"[^0-9a-zA-Z]\",\"\", review).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "11256818-3b60-4dd6-9bf9-a74160bd0954",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "부정적인 리뷰입니다.\n"
     ]
    }
   ],
   "source": [
    "review_encoding = []\n",
    "for w in review.split():\n",
    "    index = word_to_index.get(w, 2)\n",
    "    if index <= 10000:\n",
    "        review_encoding.append(index)\n",
    "    else:\n",
    "        review_encoding.apeend(word_to_index[\"UNK\"])\n",
    "        \n",
    "# 2차원 리스트로 전달하여야 한다.\n",
    "test_input = pad_sequences([review_encoding], maxlen=100)\n",
    "value = model.predict(test_input)\n",
    "if(value>0.5):\n",
    "    print('긍정적인 리뷰입니다.')\n",
    "else:\n",
    "    print('부정적인 리뷰입니다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "baef4bf0-001b-4270-a627-9bf6e7bc0bf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_index.get('the')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "790cab2a-bbdd-4259-8109-8b3c17790dfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_15 (Embedding)    (None, 100, 64)           640000    \n",
      "                                                                 \n",
      " lstm_21 (LSTM)              (None, 16)                5184      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 64)                1088      \n",
      "                                                                 \n",
      " dropout_10 (Dropout)        (None, 64)                0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 646,337\n",
      "Trainable params: 646,337\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import LSTM\n",
    "lstm_model = Sequential()\n",
    "lstm_model.add(Embedding(vocab_size, 64, input_length=100))\n",
    "lstm_model.add(LSTM(16))\n",
    "lstm_model.add(Dense(64, activation='relu'))\n",
    "lstm_model.add(Dropout(0.5))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "f45a8799-7115-4136-856c-f4a704a65208",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model.compile( loss='binary_crossentropy', optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "f50a141c-ad5b-467b-81b3-2d7e2cf637a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "391/391 [==============================] - 8s 15ms/step - loss: 0.4527 - accuracy: 0.7795 - val_loss: 0.3502 - val_accuracy: 0.8498\n",
      "Epoch 2/20\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.2615 - accuracy: 0.8974 - val_loss: 0.3696 - val_accuracy: 0.8345\n",
      "Epoch 3/20\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1834 - accuracy: 0.9316 - val_loss: 0.4233 - val_accuracy: 0.8404\n",
      "Epoch 4/20\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.1338 - accuracy: 0.9528 - val_loss: 0.5065 - val_accuracy: 0.8347\n",
      "Epoch 5/20\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.0965 - accuracy: 0.9656 - val_loss: 0.5720 - val_accuracy: 0.8334\n",
      "Epoch 6/20\n",
      "391/391 [==============================] - 5s 13ms/step - loss: 0.0754 - accuracy: 0.9741 - val_loss: 0.6280 - val_accuracy: 0.8270\n"
     ]
    }
   ],
   "source": [
    "earlystop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "history = lstm_model.fit(x_train, y_train, batch_size=64, epochs=20, verbose=1,\n",
    "                    validation_data=(x_test,y_test),callbacks=[earlystop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "bf6c7b31-3f56-4f92-8596-427b599adb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "782/782 - 3s - loss: 0.6280 - accuracy: 0.8270 - 3s/epoch - 4ms/step\n",
      "[0.6280211806297302, 0.8270000219345093]\n"
     ]
    }
   ],
   "source": [
    "result = lstm_model.evaluate(x_test, y_test, verbose=2)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow] *",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
