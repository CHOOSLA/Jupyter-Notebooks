get_ipython().getoutput("wget http://www.timeseriesclassification.com/Downloads/FordA.zip")


get_ipython().getoutput("unzip FordA.zip")


import numpy as np
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import SimpleRNN
from tensorflow.keras.layers import Dense
import matplotlib.pyplot as plt
import pandas as pd


def make_sample(data, window):
    train = []
    target = []
    for i in range(len(data)-window):
        train.append(data[i:i+window])
        target.append(data[i+window])
    return np.array(train), np.array(target)


seq_data = []
for i in np.arange(0,1000):
    seq_data += [[np.sin( np.pi * i * 0.01)]]
X, y = make_sample(seq_data, 10)


model = Sequential()
model.add(SimpleRNN(10, activation='tanh', input_shape=(10,1)))
model.add(Dense(1, activation='tanh'))
model.compile(optimizer='adam', loss='mse')


history = model.fit(X, y, epochs=100, verbose=1)
plt.plot(history.history['loss'], label='loss')
plt.show()


seq_data = []
for i in np.arange(0, 1000):
    seq_data += [[np.cos(np.pi*i*0.01)]]

X, y = make_sample(seq_data, 10)

y_pred = model.predict(X, verbose=0)
plt.plot(np.pi*np.arange(0, 990)*0.01, y_pred)
plt.plot(np.pi*np.arange(0, 990)*0.01, y)
plt.show()


def readucr(filename):
    data = np.loadtxt(filename, delimiter="\t")
    print(data.shape)
    y = data[:, 0]
    x = data[:, 1:]
    return x, y.astype(int)


root_url = "https://raw.githubusercontent.com/hfawaz/cd-diagram/master/FordA/"

x_train, y_train = readucr(root_url + "FordA_TRAIN.tsv")
x_test, y_test = readucr(root_url + "FordA_TEST.tsv")


print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)


print(y_train[5])


classes = np.unique(y_train, axis=0)

plt.figure()
for c in classes:
    c_x_train = x_train[y_train == c]
    plt.plot(c_x_train[0], label="class " + str(c))
plt.legend(loc="best")
plt.show()
plt.close()


# 데이터 전처리
## 데이터 표준화


# 우리의 타임시리즈는 이미 하나의 길이이지만, 그들의 값들은 다양한 길이를 가지고 있다.
# 이것은 뉴럴 네트워크에 적합하지 않다. 보통 우리는 입력값을 표준화할 필요가 있다.
# 이러한 데이터셋은 특히, z-normalized된 것인데, 각각의 타임시리즈 샘플들은
# 평균이 0이고 표준편차는 1이다.
# 이러한 타입의 표준화는 타임시리즈 분류에 매우 보현화되어 있다.

# 여기에 사용된 시계열 데이터는 단변량이다.
# 즉, 시계열 예제당 하나의 채널만 있음을 의미한다는 것이다.
# 우리는 그러므로 numpy를 통해 간단한 reshaping을 사용하여
# 하나의 채널이 있는 다변수 시계열로 변환시킬 것이다.
x_train = x_train.reshape((x_train.shape[0], x_train.shape[1], 1))
x_test = x_test.reshape((x_test.shape[0], x_test.shape[1], 1))
print(x_train.shape)
print(y_train.shape)
print(x_test.shape)
print(y_test.shape)


# spase_categorical_crossentropy를 사용하기 위해 class의 갯수를 저장한다.
num_classes = len(np.unique(y_train))


# np.random.permutation은 무작위로 섞인 배열을 만든다.
idx = np.random.permutation(len(x_train))

# 넘파이의 인덱스 연산을 사용한다.
x_train = x_train[idx]
y_train = y_train[idx]


# 라벨들을 양의 정수로 바꿔준다.
y_train[y_train == -1] = 0
y_test[y_test == -1] = 0


print(y_train)


# 모델 설계
from tensorflow import keras

def make_model(input_shape):
    input_layer = keras.layers.Input(input_shape)

    conv1 = keras.layers.Conv1D(filters=64, kernel_size=3, padding="same")(input_layer)
    conv1 = keras.layers.BatchNormalization()(conv1)
    conv1 = keras.layers.ReLU()(conv1)

    conv2 = keras.layers.Conv1D(filters=64, kernel_size=3, padding="same")(conv1)
    conv2 = keras.layers.BatchNormalization()(conv2)
    conv2 = keras.layers.ReLU()(conv2)

    conv3 = keras.layers.Conv1D(filters=64, kernel_size=3, padding="same")(conv2)
    conv3 = keras.layers.BatchNormalization()(conv3)
    conv3 = keras.layers.ReLU()(conv3)

    gap = keras.layers.GlobalAveragePooling1D()(conv3)

    output_layer = keras.layers.Dense(num_classes, activation="softmax")(gap)

    return keras.models.Model(inputs=input_layer, outputs=output_layer)


model = make_model(input_shape=x_train.shape[1:])
keras.utils.plot_model(model, show_shapes=True)


epochs = 500
batch_size = 32

callbacks = [
    keras.callbacks.ReduceLROnPlateau(
        monitor="val_loss", factor=0.5, patience=20, min_lr=0.0001
    ),
    keras.callbacks.EarlyStopping(monitor="val_loss", patience=50, verbose=1),
]
model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    
    ics=["sparse_categorical_accuracy"],
)
history = model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_split=0.2,
    verbose=1,
)


test_loss, test_acc = model.evaluate(x_test, y_test)

print("Test accuracy", test_acc)
print("Test loss", test_loss)


metric = "sparse_categorical_accuracy"
plt.figure()
plt.plot(history.history[metric])
plt.plot(history.history["val_" + metric])
plt.title("model " + metric)
plt.ylabel(metric, fontsize="large")
plt.xlabel("epoch", fontsize="large")
plt.legend(["train", "val"], loc="best")
plt.show()
plt.close()


def make_LSTM_model(input_shape):
    input_layer = keras.layers.Input(input_shape)

    lstm1 = keras.layers.LSTM(100, activation='tanh', return_sequnces=False)(input_layer)
    
    output_layer = keras.layers.Dense(num_classes, activation="softmax")(lstm1)

    return keras.models.Model(inputs=input_layer, outputs=output_layer)


lstm_model = make_model(input_shape=x_train.shape[1:])


lstm_model.compile(
    optimizer="adam",
    loss="sparse_categorical_crossentropy",
    metrics=["sparse_categorical_accuracy"],
)


history = lstm_model.fit(
    x_train,
    y_train,
    batch_size=batch_size,
    epochs=epochs,
    callbacks=callbacks,
    validation_split=0.2,
    verbose=1,
)


test_loss, test_acc = lstm_model.evaluate(x_test, y_test)

# LSTM 셀 100개밖에 쓰지 않았지만 Conv 모델보다 더 좋은 결과가 나온다.
print("Test accuracy", test_acc)
print("Test loss", test_loss)


metric = "sparse_categorical_accuracy"
plt.figure()
plt.plot(history.history[metric])
plt.plot(history.history["val_" + metric])
plt.title("model " + metric)
plt.ylabel(metric, fontsize="large")
plt.xlabel("epoch", fontsize="large")
plt.legend(["train", "val"], loc="best")
plt.show()
plt.close()


## 10장 문제

from zipfile import ZipFile
import os

uri = "https://storage.googleapis.com/tensorflow/tf-keras-datasets/jena_climate_2009_2016.csv.zip"
zip_path = keras.utils.get_file(origin=uri, fname="jena_climate_2009_2016.csv.zip")
zip_file = ZipFile(zip_path)
zip_file.extractall()
csv_path = "jena_climate_2009_2016.csv"

df = pd.read_csv(csv_path)


print(df.shape)


df


titles = [
    "Pressure",
    "Temperature",
    "Temperature in Kelvin",
    "Temperature (dew point)",
    "Relative Humidity",
    "Saturation vapor pressure",
    "Vapor pressure",
    "Vapor pressure deficit",
    "Specific humidity",
    "Water vapor concentration",
    "Airtight",
    "Wind speed",
    "Maximum wind speed",
    "Wind direction in degrees",
]

feature_keys = [
    "p (mbar)",
    "T (degC)",
    "Tpot (K)",
    "Tdew (degC)",
    "rh (%)",
    "VPmax (mbar)",
    "VPact (mbar)",
    "VPdef (mbar)",
    "sh (g/kg)",
    "H2OC (mmol/mol)",
    "rho (g/m**3)",
    "wv (m/s)",
    "max. wv (m/s)",
    "wd (deg)",
]

colors = [
    "blue",
    "orange",
    "green",
    "red",
    "purple",
    "brown",
    "pink",
    "gray",
    "olive",
    "cyan",
]

date_time_key = "Date Time"


def show_raw_visualization(data):
    time_data = data[date_time_key]
    # 행이 7개, 열이 2개인 subplot을 만든다
    fig, axes = plt.subplots(
        nrows=7, ncols=2, figsize=(15, 20), dpi=80, facecolor="w", edgecolor="k"
    )
    for i in range(len(feature_keys)):
        key = feature_keys[i]
        c = colors[i % (len(colors))]
        t_data = data[key]
        t_data.index = time_data
        t_data.head()
        ax = t_data.plot(
            # axes는 figure 내에서 축을 가지는 하나의 좌표평면 같은 개념
            # 실제로 데이터가 그려지는 곳은 axes이다.
            ax=axes[i // 2, i % 2],
            color=c,
            title="{} - {}".format(titles[i], key),
            # rotation의 약자로 x축의 이름을 돌린다
            rot=25,
        )
        ax.legend([titles[i]])
    plt.tight_layout()


show_raw_visualization(df)


def show_heatmap(data):
    plt.matshow(data.corr())
    # X축을 설정해준다
    plt.xticks(range(data.shape[1]), data.columns, fontsize=14, rotation=90)
    # xaxis 의 tick를 아래로 설정한다.
    plt.gca().xaxis.tick_bottom()
    # Y축을 설정해준다
    plt.yticks(range(data.shape[1]), data.columns, fontsize=14)

    # 오른쪽 bar를 나타낸다.
    cb = plt.colorbar()
    cb.ax.tick_params(labelsize=14)
    plt.title("Feature Correlation Heatmap", fontsize=14)
    plt.show()


show_heatmap(df)


# 42만개의 데이터를 가지고 있는 이 큰 데이터들을 처리해보자
# 측정은 10분에 한번씩 기록되어지고 있다.
# 그것은 한 시간마다 6번의 기록이 되어지고 있는 것을 의미한다.
# 우리는 60분 동안은 극적인 온도 변화가 없기 때문에 이것을 시간단위로 resampling 할 것이다.
# 우리는 과거의 720개의 timestamps를 이용하여 학습하고 ( 120 시간 )
# 72개의 timestamps를 예측해보려고 한다. ( 12시간 )

# 실수형 자료의 값의 범위는 매우 넓기 때문에 표준화할 필요가 있다
# 우리는 표준편차와 평균을 이용하여 0~1 사이의 실수형으로 바꿔주겠다

split_fraction = 0.715
train_split = int(split_fraction * int(df.shape[0]))
print('train 데이터 갯수 : ', train_split)
step = 6

past = 720
future = 72
learning_rate = 0.001
batch_size = 256
epochs = 10

# 정규화 과정
def normalize(data, train_split):
    data_mean = data[:train_split].mean(axis=0)
    data_std = data[:train_split].std(axis=0)
    return (data - data_mean) / data_std


# 방금 전에 뽑은 heatmap을 가지고 관계계수가 높은 열들만 뽑아낸다.
print(
    "The selected parameters are:",
    ", ".join([titles[i] for i in [0, 1, 5, 7, 8, 10, 11]]),
)
# 원하는 열들만 선택하는 부분
selected_features = [feature_keys[i] for i in [0, 1, 5, 7, 8, 10, 11]]
features = df[selected_features]
features.index = df[date_time_key]
features.head()


# 정규화를 한 후 보게되는 값들
features = normalize(features.values, train_split)
features = pd.DataFrame(features)
features.head()


# 트레인 데이터와 검증 데이터를 분리해준다
# 여기서 loc은 레이블 기반으로 인덱싱을 하는 메소드인데 정수를 가지고 인덱싱을 하고 있다
# 따라서 iloc을 써도 상관없는 부분이다.
train_data = features.loc[0 : train_split - 1]
val_data = features.loc[train_split:]

print(train_data.shape)
print(val_data.shape)


# 트레이닝 데이터 라벨은 792번째 부터 시작한다
# 우리는 792
start = past + future
end = start + train_split

x_train = train_data[[i for i in range(7)]]
y_train = features.iloc[start:end][[1]]

print(x_train.shape, y_train.shape)

sequence_length = int(past / step)

print(sequence_length)


x_train


dataset_train = keras.preprocessing.timeseries_dataset_from_array(
    x_train,
    y_train,
    sequence_length=sequence_length,
    sampling_rate=step,
    batch_size=batch_size,
)


x_end = len(val_data) - past - future

label_start = train_split + past + future

# 정답데이터 분리
x_val = val_data.iloc[:x_end][[i for i in range(7)]]
y_val = features.iloc[label_start:][[1]]

dataset_val = keras.preprocessing.timeseries_dataset_from_array(
    x_val,
    y_val,
    sequence_length=sequence_length,
    sampling_rate=step,
    batch_size=batch_size,
)


for batch in dataset_train.take(1):
    inputs, targets = batch

print("Input shape:", inputs.numpy().shape)
print("Target shape:", targets.numpy().shape)


x_val


y_val


inputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))
lstm_out = keras.layers.LSTM(32)(inputs)
outputs = keras.layers.Dense(1)(lstm_out)

model = keras.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss="mse",     metrics=["mse"],)
model.summary()


es_callback = keras.callbacks.EarlyStopping(monitor="val_loss", min_delta=0, patience=5)


history = model.fit(
    dataset_train,
    epochs=epochs,
    validation_data=dataset_val,
    callbacks=[es_callback],
    verbose=1
)


def visualize_loss(history, title):
    loss = history.history["loss"]
    val_loss = history.history["val_loss"]
    epochs = range(len(loss))
    plt.figure()
    plt.plot(epochs, loss, "b", label="Training loss")
    plt.plot(epochs, val_loss, "r", label="Validation loss")
    plt.title(title)
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()


visualize_loss(history, "Training and Validation Loss")


def show_plot(plot_data, delta, title):
    labels = ["History", "True Future", "Model Prediction"]
    marker = [".-", "rx", "go"]
    # 넘어온 x 축 데이터 120개의 갯수 마이너스화 시키고 range함
    # np.arange를 썼어도 된거 아닌가 싶음
    time_steps = list(range(-(plot_data[0].shape[0]), 0))

    if delta:
        future = delta
    else:
        future = 0

    plt.title(title)
    for i, val in enumerate(plot_data):
        if i:
            # 정답이랑 예측값
            if(i==2):
                print(plot_data[i])
            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])
        else:
            # 예측하기 전 데이터 들
            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])
    plt.legend()
    # x값의 범위
    plt.xlim([time_steps[0], (future + 5) * 2])
    plt.xlabel("Time-Step")
    plt.show()
    return


for x, y in dataset_val.take(5):
    # x.shape = (256, 120, 7)
    # x[0] = (120, 7)
    # x[0][:,1] = (120, ) temparture 데이터를 뽑아옴
    # x축은 시간 데이터
    # y축은 온도
    show_plot(
        [x[0][:, 1].numpy(), y[0].numpy(), model.predict(x)[0]],
        12,
        "Single Step Prediction",
    )




model.predict(x)
